{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b941579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "## Basic libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "## Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure libraries\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02af628",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\mayan\\OneDrive\\Desktop\\Placement Projects\\classification project\\bank.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d16ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'duration' column\n",
    "#data = data.drop('duration', axis=1)\n",
    "\n",
    "# print(df_bank.info())\n",
    "#print('Shape of dataframe:', data.shape)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da3a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98aa137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(variable):\n",
    "    var =data[variable]\n",
    "    varValue = var.value_counts()\n",
    "    plt.figure(figsize=(15,3))\n",
    "    plt.bar(varValue.index, varValue,color=['#00008b','#00e5ee','#cd1076', '#008080','#cd5555','red','blue',])\n",
    "    plt.xticks(varValue.index, varValue.index.values)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(variable)\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"{}: \\n {}\".format(variable,varValue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryc = [\"job\",\"marital\",\"education\", \"housing\", \"loan\",\"contact\",\"poutcome\",\"month\",\"deposit\"]\n",
    "for c in categoryc:\n",
    "    bar_plot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(variable):\n",
    "    plt.figure(figsize=(9,6))\n",
    "    plt.hist(data[variable], bins=40,color='#cd1076')\n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.title(\"{} distrubition with hist\".format(variable))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d248d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericVar = [\"age\",\"campaign\"]\n",
    "for n in numericVar:\n",
    "    plot_hist(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.age,data.deposit).plot(kind=\"area\",figsize=(15,7),color=['#0000ff','#000000' ])\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24130bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data.job,data.deposit).plot(kind=\"barh\",figsize=(15,7),color=['#0000ff','#000000'])\n",
    "plt.title('Deposit Age Distribution')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Job')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ea30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def detect_outliers(data,features):\n",
    "    outlier_indices = []\n",
    "    for c in features:\n",
    "        # 1st quartile\n",
    "        Q1 = np.percentile(data[c],25)\n",
    "        # 3rd quartile\n",
    "        Q3 = np.percentile(data[c],75)\n",
    "        # IQR\n",
    "        IQR = Q3 - Q1\n",
    "        # Outlier step\n",
    "        outlier_step = IQR * 1.5\n",
    "        # detect outlier and their indeces\n",
    "        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n",
    "        # store indeces\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "    \n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n",
    "    \n",
    "    return multiple_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d7cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[detect_outliers(data,['age','day','campaign','previous'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d376567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_data = data.select_dtypes(include=[float, int])\n",
    "\n",
    "# Generate the heatmap\n",
    "fig, ax = plt.subplots(figsize=(13, 13))\n",
    "sns.heatmap(numeric_data.corr(), annot=True, linewidths=.5, ax=ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8689de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdayswork(pdays):\n",
    "    if(pdays == -1):\n",
    "        return(0)\n",
    "    elif(pdays >= 0):\n",
    "        return(1)\n",
    "data['pdays'] = data['pdays'].apply(pdayswork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Copying original dataframe\n",
    "data1 = data.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['age', 'balance', 'day', 'campaign', 'previous']\n",
    "data1[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "\n",
    "data1.head()\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['age', 'balance', 'day', 'campaign', 'previous']\n",
    "data1[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Updated from sparse=False to sparse_output=False for compatibility\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Encode Categorical Data\n",
    "df_encoded = pd.DataFrame(encoder.fit_transform(data1[cat_cols]))\n",
    "df_encoded.columns = encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "# Replace Categorical Data with Encoded Data\n",
    "data1 = data1.drop(cat_cols, axis=1)\n",
    "data1 = pd.concat([df_encoded, data1], axis=1)\n",
    "\n",
    "# Encode target value\n",
    "data1['deposit'] = data1['deposit'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "print('Shape of dataframe:', data1.shape)\n",
    "data1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab623ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a189cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.iloc[:,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a495eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "# Split data\n",
    "X = data1.iloc[:, 0:51]\n",
    "Y = data1.iloc[:, 51]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n",
    "\n",
    "# Initialize dictionaries\n",
    "accuracies = {}\n",
    "kappaScores = {}\n",
    "f1scores = {}\n",
    "recallScores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef45c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c06f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b51547",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, cohen_kappa_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Optimize the pipeline for speed\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(interaction_only=True, include_bias=False)),  # Only interaction features\n",
    "    ('logreg', LogisticRegression(max_iter=200, solver='saga', warm_start=True, n_jobs=-1))  # Fastest solver with warm start\n",
    "])\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_distributions = {\n",
    "    'poly__degree': [3],  # Reduce range of polynomial degrees to speed up\n",
    "    'logreg__C': np.logspace(-2, 2, 6),  # Slightly reduce the grid size\n",
    "    'logreg__penalty': ['l1', 'l2', 'elasticnet'],  # Focus on penalties supported by saga solver\n",
    "    'logreg__max_iter': [300]  # Limit max_iter range\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=pipeline, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=30,  # Reduce the number of random combinations\n",
    "                                   cv=3,  # Reduce the number of folds for faster execution\n",
    "                                   n_jobs=-1, \n",
    "                                   scoring='accuracy',\n",
    "                                   random_state=42,\n",
    "                                   verbose=3)  # Set verbosity to 3 to print progress for each iteration\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Predict using the best model\n",
    "best_model = random_search.best_estimator_\n",
    "predictions_test = best_model.predict(X_test)\n",
    "predictions_train = best_model.predict(X_train)\n",
    "\n",
    "# Test accuracy\n",
    "test_acc = accuracy_score(y_test, predictions_test) * 100\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Train accuracy\n",
    "train_acc = accuracy_score(y_train, predictions_train) * 100\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "\n",
    "# Recall score on test data\n",
    "recall = recall_score(y_test, predictions_test, average='weighted') * 100\n",
    "print(\"Recall Score:\", recall)\n",
    "\n",
    "# F1-Score on test data\n",
    "f1 = f1_score(y_test, predictions_test, average='weighted') * 100\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Cohen Kappa score on test data\n",
    "cohen_kappa = cohen_kappa_score(y_test, predictions_test) * 100\n",
    "print('Cohen Kappa Score:', cohen_kappa)\n",
    "\n",
    "# Classification report on test data\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90446e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56257d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(bootstrap=True, max_features= 'sqrt', min_samples_leaf= 1, min_samples_split= 2, n_estimators= 2000)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "prediction = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test,prediction)*100\n",
    "print(\"Random Forest accuracy:\",acc)\n",
    "accuracies['Random Forest']=acc\n",
    "\n",
    "f1=f1_score(y_test,prediction)*100\n",
    "print(\"F1-Score: \",f1)\n",
    "f1scores['Random Forest']=f1\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction)*100\n",
    "print('Cohen Kappa score: ',cohen_kappa)\n",
    "kappaScores['Random Forest']=cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8a4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a more focused parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 2000],  # Reduced range\n",
    "    'max_depth': [10, 20, 30],   # Reduced range\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Initialize the RandomizedSearchCV with reduced iterations and verbosity\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,\n",
    "                                   n_iter=20, cv=5, verbose=2, random_state=50, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from RandomizedSearchCV\n",
    "print(\"\\nBest parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Train accuracy\n",
    "train_pred = random_search.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_pred) * 100\n",
    "train_recall = recall_score(y_train, train_pred) * 100\n",
    "print(\"\\nTrain Accuracy: {:.2f}%\".format(train_acc))\n",
    "print(\"Train Recall: {:.2f}%\".format(train_recall))\n",
    "\n",
    "# Test accuracy\n",
    "test_pred = random_search.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred) * 100\n",
    "test_recall = recall_score(y_test, test_pred) * 100\n",
    "print(\"\\nTest Accuracy: {:.2f}%\".format(test_acc))\n",
    "print(\"Test Recall: {:.2f}%\".format(test_recall))\n",
    "\n",
    "# Store accuracies and recall scores\n",
    "accuracies['Random Forest'] = test_acc\n",
    "recall_scores['Random Forest'] = test_recall\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_test, test_pred) * 100\n",
    "print(\"\\nF1-Score: {:.2f}%\".format(f1))\n",
    "f1scores['Random Forest'] = f1\n",
    "\n",
    "# Cohen Kappa Score\n",
    "cohen_kappa = cohen_kappa_score(y_test, test_pred) * 100\n",
    "print(\"Cohen Kappa score: {:.2f}%\".format(cohen_kappa))\n",
    "kappaScores['Random Forest'] = cohen_kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f43054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train,y_train)\n",
    "naiveb=nb.predict(X_test)\n",
    "prediction= nb.predict(X_test)\n",
    "acc = accuracy_score(y_test,prediction)*100\n",
    "print(\"Naive Bayes accuracy:\",acc)\n",
    "accuracies['Naive Bayes']=acc\n",
    "\n",
    "f1=f1_score(y_test,prediction)*100\n",
    "print(\"F1-Score: \",f1)\n",
    "f1scores['Naive Bayes']=f1\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction)*100\n",
    "print('Cohen Kappa score: ',cohen_kappa)\n",
    "kappaScores['Naive Bayes']=cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde444e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec90944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Naive bayes\n",
    "# Finding Accuracy, AUC, False positive rate, True positive rate, confusion matrix and classificatio report\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "pred = gnb.predict(X_test)\n",
    "accNB = accuracy_score(y_test, pred)\n",
    "y_pred_prob = gnb.predict_proba(X_test)\n",
    "aucScoreNB = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprNB, tprNB, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"AUC score for NB is \",aucScoreNB)\n",
    "print(\"Test Accuracy score for NB is \",accuracy_score(y_test, pred))\n",
    "predT=gnb.predict(X_train)\n",
    "print(\"Train Accuracy score for NB is \",accuracy_score(y_train, predT))\n",
    "#print(\"Best parameters for NB are \",gnb.best_params_)\n",
    "print(\"Recall score for NB is \",recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46c59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336dce50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd=SGDClassifier(loss='modified_huber',shuffle=True,random_state=100,penalty='l1',alpha=0.004\n",
    "                  ,max_iter=100,eta0=0.2,learning_rate='optimal')\n",
    "sgd.fit(X_train,y_train)\n",
    "prediction=sgd.predict(X_test)\n",
    "acc = accuracy_score(y_test,prediction)*100\n",
    "print(\"SGD Classifier accuracy:\",acc)\n",
    "accuracies['SGDC']=acc\n",
    "\n",
    "f1=f1_score(y_test,prediction)*100\n",
    "print(\"F1-Score: \",f1)\n",
    "f1scores['SGDC']=f1\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction)*100\n",
    "print('Cohen Kappa score: ',cohen_kappa)\n",
    "kappaScores['SGDC']=cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5dede8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb783e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn= KNeighborsClassifier(n_neighbors =3,algorithm='ball_tree')\n",
    "knn.fit(X_train, y_train)\n",
    "prediction=knn.predict(X_test)\n",
    "acc = accuracy_score(y_test,prediction)*100\n",
    "print(\"Knn accuracy:\",acc)\n",
    "accuracies['KNN']=acc\n",
    "\n",
    "f1=f1_score(y_test,prediction)*100\n",
    "print(\"F1-Score: \",f1)\n",
    "f1scores['KNN']=f1\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction)*100\n",
    "print('Cohen Kappa score: ',cohen_kappa)\n",
    "kappaScores['KNN']=cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbedc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac202808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(probability=True)\n",
    "parameters=[{'C':[1,10,100,1000],'kernel':['linear']},\n",
    "            {'C':[1,10,100,1000],'kernel':['rbf'], 'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]},\n",
    "            {'C':[1,10,100,1000],'kernel': ['poly'], 'degree':[2,3,4]}      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ff3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(estimator=classifier,param_grid=parameters,scoring='accuracy',cv=5)\n",
    "best_clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54effa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3da699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bf300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Define the SVC classifier\n",
    "classifier = SVC(probability=True)\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_distributions = {\n",
    "    'C': [1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': uniform(0.1, 0.9),  # Using uniform distribution for gamma\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "clf = RandomizedSearchCV(estimator=classifier, \n",
    "                         param_distributions=param_distributions, \n",
    "                         n_iter=20,  # Number of parameter settings sampled\n",
    "                         scoring='accuracy', \n",
    "                         cv=5, \n",
    "                         random_state=42, \n",
    "                         n_jobs=-1)  # Use all processors\n",
    "\n",
    "# Fit the model\n",
    "best_clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters found: \", best_clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM\n",
    "# Finding Accuracy, AUC, False positive rate, True positive rate, confusion matrix and classificatio report\n",
    "# get best parameters for retraining\n",
    "\n",
    "pred = best_clf.predict(X_test)\n",
    "accSVM = accuracy_score(y_test, pred)\n",
    "y_pred_prob = best_clf.predict_proba(X_test)\n",
    "aucScoreSVM = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprSVM, tprSVM, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"AUC score for SVM is \",aucScoreSVM)\n",
    "print(\"Test Accuracy score for SVM is \",accuracy_score(y_test, pred))\n",
    "predT=best_clf.predict(X_train)\n",
    "print(\"Train Accuracy score for SVM is \",accuracy_score(y_train, predT))\n",
    "print(\"Best parameters for SVM are \",best_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predR = best_clfR.predict(X_test)\n",
    "predRT=best_clfR.predict(X_train)\n",
    "recallSVM=recall_score(y_test, predR)\n",
    "print(\"Test Recall score for SVM is \",recallSVM)\n",
    "print(\"Train recall score for SVM is \",recall_score(y_train, predRT))\n",
    "print(\"Best parameters for recall of SVM are \",best_clfR.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e628d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree= DecisionTreeClassifier(criterion='gini',max_depth=10,random_state=100,min_samples_leaf=10)\n",
    "dtree.fit(X_train, y_train)\n",
    "prediction=dtree.predict(X_test)\n",
    "acc = accuracy_score(y_test,prediction)*100\n",
    "print(\"Decision Tree accuracy:\",acc)\n",
    "accuracies['Decision Tree']=acc\n",
    "\n",
    "f1=f1_score(y_test,prediction)*100\n",
    "print(\"F1-Score: \",f1)\n",
    "f1scores['Decision Tree']=f1\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction)*100\n",
    "print('Cohen Kappa score: ',cohen_kappa)\n",
    "kappaScores['Decision Tree']=cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.8,\n",
    "    max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "prediction = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, prediction)*100\n",
    "print(\"Gradient Boosting Classifier accuracy:\",acc)\n",
    "accuracies['Gradient Boosting']=acc\n",
    "\n",
    "f1=f1_score(y_test,prediction)*100\n",
    "print(\"F1-Score: \",f1)\n",
    "f1scores['Gradient Boosted']=f1\n",
    "\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction)*100\n",
    "print('Cohen Kappa score: ',cohen_kappa)\n",
    "kappaScores['Gradient Boosting']=cohen_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, cohen_kappa_score\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, \n",
    "                    subsample=0.78, colsample_bytree=1, max_depth=7)\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "prediction = xgb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy_score(y_test, prediction) * 100\n",
    "print(\"XGBoost Classifier accuracy:\", acc)\n",
    "accuracies['XGBoost Classifier'] = acc\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, prediction) * 100\n",
    "print(\"F1 Score: \", f1)\n",
    "f1scores['XGBoost Classifier'] = f1\n",
    "\n",
    "# Calculate Recall score\n",
    "recall = recall_score(y_test, prediction) * 100\n",
    "print(\"Recall Score: \", recall)\n",
    "recallScores['XGBoost Classifier'] = recall\n",
    "\n",
    "# Calculate Cohen Kappa score\n",
    "cohen_kappa = cohen_kappa_score(y_test, prediction) * 100\n",
    "print('Cohen Kappa score: ', cohen_kappa)\n",
    "kappaScores['XGBoost Classifier'] = cohen_kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,3))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"\\n\\n Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcddb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,3))\n",
    "plt.ylabel(\"Kappa Score %\")\n",
    "plt.xlabel(\"\\n\\n Algorithms\")\n",
    "sns.barplot(x=list(kappaScores.keys()), y=list(kappaScores.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,3))\n",
    "plt.ylabel(\"F1 Score %\")\n",
    "plt.xlabel(\"\\n\\n Algorithms\")\n",
    "sns.barplot(x=list(f1scores.keys()), y=list(f1scores.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "            'eta': np.arange(0.1, 0.26, 0.05),\n",
    "            'min_child_weight': np.arange(1, 5, 0.5).tolist(),\n",
    "            'gamma': [5],\n",
    "            'subsample': np.arange(0.5, 1.0, 0.11).tolist(),\n",
    "            'colsample_bytree': np.arange(0.5, 1.0, 0.11).tolist()\n",
    "        }\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee29441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'random_search' is the RandomizedSearchCV object used for hyperparameter tuning\n",
    "\n",
    "# Extracting the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Training the model with the best parameters\n",
    "model = XGBClassifier(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Cohen's Kappa Score: {kappa:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance  # Import plot_importance separately\n",
    "\n",
    "# Assuming you have trained an XGBoost model\n",
    "# model = xgb.XGBClassifier(...)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_importance(model, importance_type='weight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['previous']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274efa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have trained a model and X_train is your training data\n",
    "# model = xgb.XGBClassifier(...)\n",
    "# X_train = ...\n",
    "\n",
    "# Create PDP for specific features\n",
    "features = ['previous']  # Replace with indices or names of features\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, features, ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df contains your data with a binary target variable 'subscribed'\n",
    "# and a predictor 'predictor_name'\n",
    "\n",
    "# Bin the predictor values if needed\n",
    "data['predictor_binned'] = pd.qcut(data['previous'], q=10)  # Binning into 10 quantiles\n",
    "\n",
    "# Calculate conversion rate by predictor value\n",
    "conversion_rate = data.groupby('previous')['deposit'].mean()\n",
    "\n",
    "# Plot conversion rates\n",
    "conversion_rate.plot(kind='bar')\n",
    "plt.xlabel('Predictor Value Binned')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.title('Conversion Rate by Predictor Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4482d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "# Example: Partial dependence for 'balance'\n",
    "pd_results = partial_dependence(\n",
    "    model, X_train, features=[X_train.columns.get_loc('balance')]\n",
    ")\n",
    "\n",
    "# Extract the partial dependence values\n",
    "pd_values = pd_results['average']  # For scikit-learn version 0.24 and later\n",
    "\n",
    "print(\"Partial Dependence values for 'balance':\")\n",
    "print(pd_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8573ad1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'model' is your trained model and 'X_train' is your training data\n",
    "\n",
    "features = ['previous', 'poutcome_success']  # Replace with the features you want to analyze\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, features, grid_resolution=20)\n",
    "\n",
    "plt.suptitle('Partial Dependence of Balance and Age on Subscription Probability')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Bin features if necessary\n",
    "data1['balance_binned'] = pd.qcut(data['balance'], q=10)\n",
    "data1['age_binned'] = pd.qcut(data['age'], q=10)\n",
    "\n",
    "# Calculate conversion rates\n",
    "combination_rates = data1.groupby(['balance_binned', 'age_binned'])['deposit'].mean().reset_index()\n",
    "\n",
    "# Pivot table for heatmap\n",
    "pivot_table = combination_rates.pivot('balance_binned', 'age_binned', 'deposit')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Conversion Rate by Combination of Balance and Age')\n",
    "plt.xlabel('Age Binned')\n",
    "plt.ylabel('Balance Binned')\n",
    "plt.imshow(pivot_table, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Conversion Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a92312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame (replace with your actual data)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Binning the balance into quantiles\n",
    "data1['balance_binned'] = pd.qcut(data1['balance'], q=10, labels=False)\n",
    "\n",
    "# Calculate conversion rate by balance bin and credit default status\n",
    "conversion_rate = data1.groupby(['balance_binned', 'default_yes'])['deposit'].mean().unstack()\n",
    "\n",
    "# Plotting\n",
    "conversion_rate.plot(kind='bar', figsize=(12, 6))\n",
    "plt.xlabel('Balance Bin')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.title('Conversion Rate by Balance and Credit Default Status')\n",
    "plt.legend(title='Credit Default')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace with your actual data)\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Bin the balance into quantiles\n",
    "bins = pd.qcut(data1['balance'], q=10, retbins=True)[1]  # Get bin edges\n",
    "\n",
    "# Print the bin edges\n",
    "print(\"Bin edges for 'balance':\")\n",
    "print(bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b54e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming bins have been calculated as above\n",
    "bin_index = 6  # Bin 7 corresponds to index 6\n",
    "\n",
    "# Define the range for bin 7\n",
    "bin_range = (bins[bin_index], bins[bin_index + 1])\n",
    "\n",
    "# Filter the data that falls into bin 7\n",
    "df_bin7 = data1[(data1['balance'] > bin_range[0]) & (data1['balance'] <= bin_range[1])]\n",
    "\n",
    "# Display the data for bin 7\n",
    "print(f\"Data for bin 7 (Balance between {bin_range[0]} and {bin_range[1]}):\")\n",
    "print(df_bin7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data['balance'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6449d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['balance'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=y**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.2064753533504524*z+x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.058239648485650324*z+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb762b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Plot SHAP dependence plot for a specific feature\n",
    "shap.dependence_plot(\"feature_name\", shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1)\n",
    "}\n",
    "\n",
    "# Define the objective function to minimize\n",
    "def objective(params):\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Perform the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=100)\n",
    "print(\"Best set of hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hypercot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
